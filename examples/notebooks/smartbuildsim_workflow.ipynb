{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SmartBuildSim Workflow Notebook\n\n",
        "This notebook mirrors the [`examples/scripts/run_example.py`](../scripts/run_example.py) workflow using the public SmartBuildSim APIs.\n",
        "It walks through synthetic data generation, model training, anomaly detection, clustering, reinforcement learning, and visualisation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install dependencies\n\n",
        "Run the same installation command documented in the [Quickstart guide](../../docs/quickstart.md) before executing the notebook:\n\n",
        "```bash\n",
        "pip install -e .[dev]\n",
        "```\n\n",
        "The extras include Jupyter, plotting, and testing dependencies so every cell can run without additional setup."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure scenario and outputs\n\n",
        "This example uses the built-in `office-small` preset, writing all artefacts to `examples/outputs/`.\n",
        "Re-run cells to regenerate artefacts; existing files will be overwritten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from smartbuildsim.data.generator import DataGeneratorConfig, generate_dataset\n",
        "from smartbuildsim.models.anomaly import AnomalyDetectionConfig, detect_anomalies\n",
        "from smartbuildsim.models.clustering import ClusteringConfig, cluster_zones\n",
        "from smartbuildsim.models.forecasting import ForecastingConfig, train_forecasting_model\n",
        "from smartbuildsim.models.rl import RLConfig, train_policy\n",
        "from smartbuildsim.scenarios.presets import get_scenario\n",
        "from smartbuildsim.viz.plots import PlotConfig, plot_time_series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_dir = Path('examples/outputs')\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "scenario = get_scenario('office-small')\n",
        "data_config = DataGeneratorConfig(**scenario.data.dict())\n",
        "forecast_config = ForecastingConfig(**scenario.forecasting.dict())\n",
        "anomaly_config = AnomalyDetectionConfig(**scenario.anomaly.dict())\n",
        "cluster_config = ClusteringConfig(**scenario.clustering.dict())\n",
        "rl_config = RLConfig(**scenario.rl.dict())\n",
        "plot_config = PlotConfig(sensor=scenario.forecasting.sensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate synthetic telemetry\n\n",
        "Use the `DataGeneratorConfig` to sample deterministic building telemetry and persist it for downstream steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = generate_dataset(scenario.building, data_config)\n",
        "dataset_path = output_dir / 'dataset.csv'\n",
        "dataset.to_csv(dataset_path, index=False)\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train forecasting model\n\n",
        "Fit the forecasting model defined by the preset and inspect a subset of predictions alongside the RMSE metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "forecast_result = train_forecasting_model(dataset, forecast_config)\n",
        "forecast_summary = {\n",
        "    'rmse': forecast_result.rmse,\n",
        "    'predictions': forecast_result.predictions[:5].tolist(),\n",
        "}\n",
        "forecast_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detect anomalies\n\n",
        "The anomaly detector flags unusual telemetry points. The resulting dataframe is useful for plotting and further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "anomaly_result = detect_anomalies(dataset, anomaly_config)\n",
        "len(anomaly_result.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cluster building zones\n\n",
        "Group zones using the clustering configuration shipped with the scenario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster_result = cluster_zones(dataset, cluster_config)\n",
        "cluster_result.assignments.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train reinforcement learning policy\n\n",
        "Optimise the RL control policy and report the mean episodic reward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rl_result = train_policy(rl_config)\n",
        "rl_result.average_reward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualise sensor telemetry\n\n",
        "Plot the target sensor with anomaly annotations. The file is written to `examples/outputs/sensor_plot.png`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_path = output_dir / 'sensor_plot.png'\n",
        "plot_time_series(dataset, plot_config, plot_path, anomalies=anomaly_result.data)\n",
        "plot_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summarise outputs\n\n",
        "Collect key metrics and artefact locations for quick reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary = pd.DataFrame(\n",
        "    {\n",
        "        'forecast_rmse': [forecast_result.rmse],\n",
        "        'avg_rl_reward': [rl_result.average_reward()],\n",
        "        'clusters': [cluster_result.assignments.to_dict(orient='records')],\n",
        "    }\n",
        ")\n",
        "summary_path = output_dir / 'summary.csv'\n",
        "summary.to_csv(summary_path, index=False)\n",
        "print(f'Dataset saved to {dataset_path}')\n",
        "print(f'Forecast summary: {forecast_summary}')\n",
        "print(f'Anomaly output rows: {len(anomaly_result.data)}')\n",
        "print(f'Cluster assignments saved with {len(cluster_result.assignments)} entries')\n",
        "print(f'RL mean reward: {rl_result.average_reward():.3f}')\n",
        "print(f'Plot saved to {plot_path}')\n",
        "print(f'Summary saved to {summary_path}')\n",
        "summary"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}